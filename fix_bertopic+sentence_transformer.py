# -*- coding: utf-8 -*-
"""FIX BERTOPIC+SENTENCE TRANSFORMER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CXLdNqMoUUOvIPkotAjbyxGjclmihGnO
"""

!pip install tensorflowjs --no-cache-dir

!pip install bertopic
!pip install sentence-transformers
!pip install gensim
!pip install nltk
!pip install Sastrawi

from google.colab import drive
drive.mount('/content/drive')

"""#Import Library"""

import nltk
import string
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.sparse as sp
import re

from bertopic import BERTopic
from gensim.models import Word2Vec
from gensim import corpora
from gensim.models.coherencemodel import CoherenceModel
from sentence_transformers import SentenceTransformer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from tqdm import tqdm

"""#Download Package tambahan NLTK"""

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('punkt_tab')

"""#Read Data

"""

data1 = pd.read_csv('/content/drive/MyDrive/nana/dataset_tiktok-comments-scraper_2025-08-30_14-48-28-486.csv')
data2 = pd.read_csv('/content/drive/MyDrive/nana/dataset_tiktok-comments-scraper_2025-08-30_14-53-31-785.csv')

data = pd.concat([data1, data2])

"""#Preprocessing (Cleaning & EDA)"""

data.duplicated().sum()

data.shape

data = data.drop_duplicates()

data.duplicated().sum()

data.shape

data.info()

data.head(10)

data.isnull().sum()

data.dropna(inplace=True)

data.shape

data.columns

data.drop(columns=['diggCount', 'replyCommentTotal', 'createTimeISO', 'uniqueId',
       'videoWebUrl', 'uid', 'cid', 'avatarThumbnail'], axis=1, inplace=True)

def preprocessText(text:str):
  text = text.lower()
  text = text.translate(str.maketrans('', '', string.punctuation))
  text = re.sub(r'[^\w\s#@/:%\.\-]', '', text)
  words = word_tokenize(text)
  stopWords = set(stopwords.words('indonesian'))
  words = [word for word in words if word not in stopWords]
  return " ".join(words)

data['afterCleaning'] = data['text'].apply(preprocessText)

data.head(10)

factory = StemmerFactory()
stemmer = factory.create_stemmer()

uniqueWords = set()
data['afterCleaning'].str.split().apply(uniqueWords.update)

stemmerWords = {word: stemmer.stem(word) for word in tqdm(uniqueWords)}

stemming = lambda text: ' '.join([stemmerWords[word] for word in text.split()])
data['afterStemming'] = data['afterCleaning'].apply(stemming)

data.head(10)

"""tesss"""

text = data['afterStemming'].tolist()

textTokenized = [text.split() for text in text]

textTokenized

!pip install indobert-st

"""## Define IndoBERT Embedding Model

### Subtask:
Define the IndoBERT Sentence Transformer model ('indolem/indobert-base-uncased') to be used for generating embeddings. This step prepares the embedding component for BERTopic.

**Reasoning**:
The subtask requires defining the IndoBERT Sentence Transformer model. This involves importing the `SentenceTransformer` class and then instantiating the model with the specified model name ('indolem/indobert-base-uncased').
"""

from sentence_transformers import SentenceTransformer

indoBERT_model = SentenceTransformer('indolem/indobert-base-uncased')
print("IndoBERT model loaded successfully.")

"""**Reasoning**:
The previous code generated a warning because `sentence_transformers` could not find a pre-packaged model with the name 'indolem/indobert-base-uncased', causing it to create a new one with mean pooling. To explicitly define the IndoBERT Sentence Transformer model and avoid this warning, it's best to construct the `SentenceTransformer` by first loading the `transformers` model and then adding a pooling layer.


"""

from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
from sentence_transformers.models import Transformer, Pooling

# 1. Load the pre-trained IndoBERT model from Hugging Face's transformers
word_embedding_model = Transformer(model_name_or_path='indolem/indobert-base-uncased')

# 2. Add a pooling layer to convert token embeddings into a single sentence embedding (mean pooling is common)
pooling_model = Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode='mean')

# 3. Combine the Transformer and Pooling models into a SentenceTransformer
indoBERT_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])

print("IndoBERT model loaded successfully using explicit construction.")

"""## Generate IndoBERT Embeddings

### Subtask:
Generate sentence embeddings for the preprocessed text data (`text`) using the defined IndoBERT model.

**Reasoning**:
To generate sentence embeddings for the preprocessed text data, I will use the `indoBERT_model.encode()` method on the `text` variable and store the result in a new variable called `indoBERT_embeddings`.
"""

indoBERT_embeddings = indoBERT_model.encode(text)
print("IndoBERT embeddings generated successfully.")

"""## Initialize and Train BERTopic Model

### Subtask:
Initialize and train the BERTopic model using the IndoBERT embeddings. Adjust `min_topic_size` (e.g., set to 30) to aim for a coherence score above 0.5. The model will be stored as `topic_model_indoBERT`.

### Instructions:
1. Import the `BERTopic` class from the `bertopic` library.
2. Initialize a `BERTopic` model instance, setting the `embedding_model` parameter to `indoBERT_model` and `min_topic_size` to 30.
3. Fit the BERTopic model to your preprocessed text data (`text`) and the generated IndoBERT embeddings (`indoBERT_embeddings`).
4. Store the trained BERTopic model in a variable named `topic_model_indoBERT`.
5. Extract the topics and probabilities from the fitted model and store them in variables named `topics` and `probabilities` respectively.

**Reasoning**:
Now, I will implement the instructions to initialize and train the BERTopic model using the IndoBERT embeddings, set the minimum topic size, fit the model, and then extract the topics and probabilities as requested in the subtask.
"""

from bertopic import BERTopic

# Initialize BERTopic model with IndoBERT embeddings and specified min_topic_size
topic_model_indoBERT = BERTopic(embedding_model=indoBERT_model, min_topic_size=30)

# Fit the model to the preprocessed text and IndoBERT embeddings
topics, probabilities = topic_model_indoBERT.fit_transform(text, embeddings=indoBERT_embeddings)

print("BERTopic model initialized and trained successfully.")
print(f"Number of topics found: {len(set(topics)) - 1}") # -1 to exclude noise topic

"""## Prepare Data for Coherence Metrics (IndoBERT Model)

### Subtask:
Prepare the necessary `gensim` dictionary and corpus, and extract topic-word distributions (`bertopic_topics_indoBERT`, `top_words_per_topic_indoBERT`) from the trained `topic_model_indoBERT` for metrics calculation.

**Reasoning**:
To prepare the data for coherence metrics, I will first create a gensim dictionary and corpus from the tokenized text. Then, I will extract topic-word distributions and topic information from the BERTopic model, and process the topic names to get the top words per topic as instructed. All these steps will be combined into a single code block to maintain clarity and efficiency.
"""

from gensim import corpora

# 1. Create a gensim dictionary from textTokenized
dictionary_indoBERT = corpora.Dictionary(textTokenized)

# 2. Create a gensim corpus from textTokenized
corpus_indoBERT = [dictionary_indoBERT.doc2bow(text) for text in textTokenized]

# 3. Extract the topic-word distributions from topic_model_indoBERT
bertopic_topics_indoBERT = topic_model_indoBERT.get_topics()

# 4. Extract the topic information from topic_model_indoBERT
topic_info_indoBERT = topic_model_indoBERT.get_topic_info()

# 5. From topic_info_indoBERT, extract the 'Name' column, clean it to get only the words for each topic
top_words_per_topic_indoBERT = [
    [word for word in topic.split('_')[1:] if word] # Exclude topic ID and count prefix
    for topic in topic_info_indoBERT['Name']
]

# 6. Print a confirmation message
print("Data preparation for coherence metrics is complete.")

"""**Reasoning**:
The subtask requires calculating the coherence score for the BERTopic model. I will use the `CoherenceModel` from `gensim` with the previously prepared `topics_indoBERT`, `corpus_indoBERT`, `dictionary_indoBERT`, and `top_words_per_topic_indoBERT` to calculate the coherence score for the IndoBERT model.


"""

from gensim.models.coherencemodel import CoherenceModel

# Prepare the topics in the required format for CoherenceModel
topics_indoBERT = [[word for word, _ in bertopic_topics_indoBERT[topic_id]] for topic_id in bertopic_topics_indoBERT if topic_id != -1]

# Calculate Coherence Score for IndoBERT model
coherence_model_indoBERT = CoherenceModel(topics=topics_indoBERT, texts=textTokenized, corpus=corpus_indoBERT, dictionary=dictionary_indoBERT, coherence='c_v')
coherence_score_indoBERT = coherence_model_indoBERT.get_coherence()

print(f"Coherence score for IndoBERT model: {coherence_score_indoBERT}")

"""## Consolidated Coherence Scores (IndoBERT Model)

This table presents both the overall Coherence Score (c_v) for the BERTopic model using IndoBERT embeddings, and the individual Coherence Scores for each topic, including their descriptive names. This provides a comprehensive overview of topic interpretability.
"""

from gensim.models.coherencemodel import CoherenceModel
import pandas as pd

# --- Overall Coherence Score (Recalculated for this cell) ---
# Prepare the topics in the required format for CoherenceModel
topics_for_overall_coherence = [[word for word, _ in bertopic_topics_indoBERT[topic_id]] for topic_id in bertopic_topics_indoBERT if topic_id != -1]

# Calculate Overall Coherence Score for IndoBERT model
coherence_model_overall = CoherenceModel(topics=topics_for_overall_coherence, texts=textTokenized, corpus=corpus_indoBERT, dictionary=dictionary_indoBERT, coherence='c_v')
overall_coherence_score = coherence_model_overall.get_coherence()

# --- Per-Topic Coherence Scores (Recalculated for this cell) ---
per_topic_coherence_data = []
for topic_id in bertopic_topics_indoBERT:
    if topic_id == -1:
        continue

    topic_words_only = [word for word, score in bertopic_topics_indoBERT[topic_id]]
    current_topic_words = [topic_words_only]

    coherence_model_per_topic = CoherenceModel(
        topics=current_topic_words,
        texts=textTokenized,
        corpus=corpus_indoBERT,
        dictionary=dictionary_indoBERT,
        coherence='c_v'
    )
    coherence_score = coherence_model_per_topic.get_coherence()

    topic_name = topic_info_indoBERT[topic_info_indoBERT['Topic'] == topic_id]['Name'].iloc[0]
    per_topic_coherence_data.append({'Topic ID': topic_id, 'Topic Name': topic_name, 'Coherence Score (c_v)': coherence_score})

# Create DataFrame for per-topic coherence
per_topic_coherence_df = pd.DataFrame(per_topic_coherence_data)

# --- Consolidated Output ---
print(f"Overall Coherence Score (c_v) for IndoBERT model: {overall_coherence_score:.4f}")
print("\nPer-topic coherence scores for IndoBERT model:")
display(per_topic_coherence_df.round(4))

"""## Calculate and Display Topic Diversity (IndoBERT Model)

### Subtask:
Calculate and print the Topic Diversity (top 10 words) for the `topic_model_indoBERT`.

**Reasoning**:
To calculate the Topic Diversity for the `topic_model_indoBERT`, I will follow the instructions to iterate through the top words of each topic, add them to a set to count unique words, calculate the total words considered, and then compute the diversity score.
"""

unique_words_in_topics = set()

# Iterate through each topic in bertopic_topics_indoBERT, excluding the noise topic (-1)
for topic_id in bertopic_topics_indoBERT:
    if topic_id == -1:
        continue

    # Extract the top 10 words for the current topic
    # bertopic_topics_indoBERT[topic_id] contains a list of (word, score) tuples
    top_10_words = [word for word, score in bertopic_topics_indoBERT[topic_id][:10]]
    unique_words_in_topics.update(top_10_words)

# Calculate the total words considered (number of topics * 10)
# Number of topics excludes the noise topic (-1)
num_topics = len(bertopic_topics_indoBERT) - 1
total_words_considered = num_topics * 10

# Calculate topic diversity
topic_diversity = len(unique_words_in_topics) / total_words_considered

print(f"Topic Diversity (top 10 words) for IndoBERT model: {topic_diversity:.4f}")

"""## Display Full Topic Information Table (IndoBERT Model)

### Subtask:
Display the full topic information table for the `topic_model_indoBERT`.

**Reasoning**:
The subtask requests displaying the full topic information table for the `topic_model_indoBERT`. This information is already stored in the `topic_info_indoBERT` DataFrame, so I will display it.
"""

print("Full Topic Information Table for IndoBERT Model:")
print(topic_info_indoBERT)

"""## Display Top 10 Topics (IndoBERT Model)

### Subtask:
Display a table of the top 10 topics identified by the `topic_model_indoBERT`, sorted by their document count.

**Reasoning**:
To display the top 10 topics by document count, I will filter out the noise topic from `topic_info_indoBERT`, sort the remaining topics by 'Count' in descending order, and then select the top 10 entries.
"""

print("Top 10 Topics (IndoBERT Model) by Document Count:")
top_10_topics_indoBERT = topic_info_indoBERT[topic_info_indoBERT.Topic != -1].sort_values(by="Count", ascending=False).head(10)
print(top_10_topics_indoBERT)

"""## Visualize Topic Word Scores (Barchart) (IndoBERT Model)

### Subtask:
Generate and display the barchart visualization showing the most representative words for a selected number of topics from the `topic_model_indoBERT`.

## Visualize Intertopic Distance Map (IndoBERT - All Topics)

This visualization shows the relationships between **all** the discovered topics, based on their embeddings using the IndoBERT model. The number of topics displayed here is determined by the BERTopic model itself during its training phase.
"""

topic_model_indoBERT.visualize_topics()

"""**Reasoning**:
To generate and display the barchart visualization of topic word scores, I will use the `visualize_barchart()` method of the `topic_model_indoBERT` object, specifying `top_n_topics=10` as requested in the instructions.


"""

topic_model_indoBERT.visualize_barchart(top_n_topics=10)

"""## Final Task

### Subtask:
Summarize the results of the IndoBERT BERTopic model, including overall and per-topic coherence scores, and topic diversity, then ask if proceeding to the Universal Sentence Encoder (USE) model is desired.

## Summary:

### Q&A
The results of the IndoBERT BERTopic model are summarized below, including overall and per-topic coherence scores, and topic diversity.

### Data Analysis Key Findings
*   The IndoBERT Sentence Transformer model (`indolem/indobert-base-uncased`) was successfully loaded and used to generate embeddings for the text data.
*   The BERTopic model, trained with IndoBERT embeddings and a `min_topic_size` of 30, identified **11 distinct topics** (excluding the noise topic).
*   The **overall coherence score (c\_v)** for the IndoBERT BERTopic model is approximately **0.4572**.
*   Per-topic coherence scores varied significantly:
    *   The highest per-topic coherence score was **0.9053** (for Topic 0).
    *   The lowest per-topic coherence score was **0.2236** (for Topic 9).
*   The **Topic Diversity (based on the top 10 words)** for the IndoBERT model is approximately **0.6727**, indicating that about 67.27% of the top 10 words across all non-noise topics are unique.
*   Topic 0 is the most prominent topic, containing **1983 documents**, while the noise topic (-1) contains 142 documents.
*   Visualizations including the intertopic distance map and barcharts of topic word scores were successfully generated.

### Insights or Next Steps
*   The overall coherence of 0.4572 is moderate, and while Topic 0 shows high coherence, other topics, like Topic 9, have low coherence, suggesting potential issues with their interpretability.
*   Would you like to proceed with evaluating the Universal Sentence Encoder (USE) model for comparison, or would you prefer to explore ways to improve the IndoBERT model's coherence (e.g., by adjusting hyperparameters like `min_topic_size` or `n_gram_range`)?
"""