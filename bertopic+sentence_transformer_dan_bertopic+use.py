# -*- coding: utf-8 -*-
"""BERTOPIC+SENTENCE TRANSFORMER dan BERTOPIC+USE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h6DyMNHgcz26rUWZBZcv93HhKg6EmQ35
"""

!pip install tensorflowjs --no-cache-dir

!pip install bertopic
!pip install sentence-transformers
!pip install gensim
!pip install nltk
!pip install Sastrawi

from google.colab import drive
drive.mount('/content/drive')

"""#Import Library"""

import nltk
import string
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.sparse as sp
import re

from bertopic import BERTopic
from gensim.models import Word2Vec
from gensim import corpora
from gensim.models.coherencemodel import CoherenceModel
from sentence_transformers import SentenceTransformer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from tqdm import tqdm

"""#Download Package tambahan NLTK"""

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('punkt_tab')

"""#Read Data

"""

data1 = pd.read_csv('/content/drive/MyDrive/nana/dataset_tiktok-comments-scraper_2025-08-30_14-48-28-486.csv')
data2 = pd.read_csv('/content/drive/MyDrive/nana/dataset_tiktok-comments-scraper_2025-08-30_14-53-31-785.csv')

data = pd.concat([data1, data2])

"""#Preprocessing (Cleaning & EDA)"""

data.duplicated().sum()

data.shape

data = data.drop_duplicates()

data.duplicated().sum()

data.shape

data.info()

data.head(10)

data.isnull().sum()

data.dropna(inplace=True)

data.shape

data.columns

data.drop(columns=['diggCount', 'replyCommentTotal', 'createTimeISO', 'uniqueId',
       'videoWebUrl', 'uid', 'cid', 'avatarThumbnail'], axis=1, inplace=True)

def preprocessText(text:str):
  text = text.lower()
  text = text.translate(str.maketrans('', '', string.punctuation))
  text = re.sub(r'[^\w\s#@/:%\.\-]', '', text)
  words = word_tokenize(text)
  stopWords = set(stopwords.words('indonesian'))
  words = [word for word in words if word not in stopWords]
  return " ".join(words)

data['afterCleaning'] = data['text'].apply(preprocessText)

data.head(10)

factory = StemmerFactory()
stemmer = factory.create_stemmer()

uniqueWords = set()
data['afterCleaning'].str.split().apply(uniqueWords.update)

stemmerWords = {word: stemmer.stem(word) for word in tqdm(uniqueWords)}

stemming = lambda text: ' '.join([stemmerWords[word] for word in text.split()])
data['afterStemming'] = data['afterCleaning'].apply(stemming)

data.head(10)

"""tesss"""

text = data['afterStemming'].tolist()

textTokenized = [text.split() for text in text]

textTokenized

!pip install indobert-st

"""##Define IndoBERT Embedding Model

**Reasoning**:
mendefinisikan model IndoBERT Sentence Transformer. Ini mencakup mengimpor kelas SentenceTransformer dan kemudian membuat instance model tersebut dengan menggunakan nama model yang sudah ditentukan.
"""

from sentence_transformers import SentenceTransformer

indoBERT_model = SentenceTransformer('indolem/indobert-base-uncased')
print("IndoBERT model loaded successfully.")

"""**Reasoning**: Kode sebelumnya menghasilkan peringatan karena sentence_transformers tidak menemukan model siap pakai dengan nama 'indolem/indobert-base-uncased', sehingga ia membuat model baru dengan mean pooling. Untuk secara eksplisit mendefinisikan model IndoBERT Sentence Transformer dan menghindari peringatan ini, cara terbaik adalah membangun SentenceTransformer dengan terlebih dahulu memuat model dari transformers lalu menambahkan lapisan pooling.

"""

from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
from sentence_transformers.models import Transformer, Pooling

# 1. Load the pre-trained IndoBERT model from Hugging Face's transformers
word_embedding_model = Transformer(model_name_or_path='indolem/indobert-base-uncased')

# 2. Add a pooling layer to convert token embeddings into a single sentence embedding (mean pooling is common)
pooling_model = Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode='mean')

# 3. Combine the Transformer and Pooling models into a SentenceTransformer
indoBERT_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])

print("IndoBERT model loaded successfully using explicit construction.")

"""## Generate IndoBERT Embeddings

**Reasoning**:
Untuk menghasilkan sentence embeddings dari data teks yang sudah dipreproses, akan digunakan metode indoBERT_model.encode() pada variabel text, lalu hasilnya disimpan ke variabel baru bernama indoBERT_embeddings.
"""

indoBERT_embeddings = indoBERT_model.encode(text)
print("IndoBERT embeddings generated successfully.")

"""## initialize BERTopic Model (IndoBERT - Coherence Focus)

### Subtask:
Re-initialize and re-train the BERTopic model using the IndoBERT embeddings. Set `min_topic_size` to a larger value (e.g., 20) to aim for higher overall coherence. The model will continue to be stored as `topic_model_indoBERT` (overwriting the previous one) and its `topics`, `probabilities`, and `topic_info_indoBERT` will be updated.

**Reasoning**:
To re-initialize and re-train the BERTopic model with a focus on higher overall coherence, I will set a larger `min_topic_size` and fit the model again using the existing IndoBERT embeddings and preprocessed text.
"""

from bertopic import BERTopic

# Initialize BERTopic model with IndoBERT embeddings and a larger min_topic_size
topic_model_indoBERT = BERTopic(embedding_model=indoBERT_model, min_topic_size=20)

# Fit the new model to the preprocessed text and IndoBERT embeddings
topics, probabilities = topic_model_indoBERT.fit_transform(text, embeddings=indoBERT_embeddings)

# Get the updated topic information
topic_info_indoBERT = topic_model_indoBERT.get_topic_info()

print("BERTopic model re-initialized and re-trained successfully with min_topic_size=20.")
print(f"Number of topics found: {len(set(topics)) - 1}") # -1 to exclude noise topic

"""**Reasoning**:
The next step is to prepare the data for coherence metrics using the newly re-trained BERTopic model. This involves re-creating the gensim dictionary and corpus, and extracting topic-word distributions and topic information from the updated `topic_model_indoBERT`, as the previous execution might not reflect the changes from the re-training.


"""

from gensim import corpora

# 1. Create a gensim dictionary from textTokenized
dictionary_indoBERT = corpora.Dictionary(textTokenized)

# 2. Create a gensim corpus from textTokenized
corpus_indoBERT = [dictionary_indoBERT.doc2bow(text) for text in textTokenized]

# 3. Extract the topic-word distributions from topic_model_indoBERT
bertopic_topics_indoBERT = topic_model_indoBERT.get_topics()

# 4. Extract the topic information from topic_model_indoBERT
topic_info_indoBERT = topic_model_indoBERT.get_topic_info()

# 5. From topic_info_indoBERT, extract the 'Name' column, clean it to get only the words for each topic
top_words_per_topic_indoBERT = [
    [word for word in topic.split('_')[1:] if word] # Exclude topic ID and count prefix
    for topic in topic_info_indoBERT['Name']
]

# 6. Print a confirmation message
print("Data preparation for coherence metrics is complete.")

"""**Reasoning**:
I will calculate and display the overall and per-topic coherence scores (c_v) using `CoherenceModel` from gensim. I will also calculate topic diversity by checking the uniqueness of the top 10 words across all topics. The results will be summarized and printed.


"""

from gensim.models.coherencemodel import CoherenceModel

# Filter out empty lists from top_words_per_topic_indoBERT to avoid errors in coherence calculation
top_words_for_coherence = [words for words in top_words_per_topic_indoBERT if words]

# Calculate overall coherence score (c_v)
coherence_model_indoBERT = CoherenceModel(topics=top_words_for_coherence, texts=textTokenized,
                                          dictionary=dictionary_indoBERT, coherence='c_v')
overall_coherence_score = coherence_model_indoBERT.get_coherence()
print(f"Overall Coherence Score (c_v): {overall_coherence_score}")

# Calculate per-topic coherence scores (c_v)
per_topic_coherence_data = []
for i, topic_words in enumerate(top_words_per_topic_indoBERT):
    if topic_words:
        topic_coherence_model = CoherenceModel(topics=[topic_words], texts=textTokenized,
                                             dictionary=dictionary_indoBERT, coherence='c_v')
        per_topic_coherence_data.append({
            'Topic ID': i,
            'Topic Name': topic_info_indoBERT['Name'].iloc[i],
            'Coherence Score (c_v)': topic_coherence_model.get_coherence()
        })

per_topic_coherence_df = pd.DataFrame(per_topic_coherence_data)
print("\nPer-Topic Coherence Scores:")
print(per_topic_coherence_df)

# Calculate topic diversity
all_top_words = set()
for topic_word_list in top_words_for_coherence:
    for word in topic_word_list:
        all_top_words.add(word)

total_words_considered = sum(len(topic_words) for topic_words in top_words_for_coherence)

# Filter out noise topic (-1) from top_words_per_topic_indoBERT for diversity calculation
# Assuming topic_info_indoBERT has -1 as the first topic (index 0) if it exists
filtered_top_words_for_diversity = []
for i, words in enumerate(top_words_per_topic_indoBERT):
    if topic_info_indoBERT['Topic'].iloc[i] != -1:
        filtered_top_words_for_diversity.append(words)

unique_words_across_topics = set()
num_total_words = 0
for words in filtered_top_words_for_diversity:
    unique_words_across_topics.update(words)
    num_total_words += len(words)

topic_diversity = len(unique_words_across_topics) / num_total_words if num_total_words > 0 else 0
print(f"\nTopic Diversity: {topic_diversity}")

print("\nCoherence and diversity calculation complete.")

from gensim.models.coherencemodel import CoherenceModel
import pandas as pd

# --- Overall Coherence Score ---
# Prepare the topics in the required format for CoherenceModel
topics_for_overall_coherence = [[word for word, _ in bertopic_topics_indoBERT[topic_id]] for topic_id in bertopic_topics_indoBERT if topic_id != -1]

# Calculate Overall Coherence Score for IndoBERT model
coherence_model_overall = CoherenceModel(topics=topics_for_overall_coherence, texts=textTokenized, corpus=corpus_indoBERT, dictionary=dictionary_indoBERT, coherence='c_v')
overall_coherence_score = coherence_model_overall.get_coherence()

# --- Per-Topic Coherence Scores ---
per_topic_coherence_data = []
for topic_id in bertopic_topics_indoBERT:
    if topic_id == -1:
        continue

    topic_words_only = [word for word, score in bertopic_topics_indoBERT[topic_id]]
    current_topic_words = [topic_words_only]

    coherence_model_per_topic = CoherenceModel(
        topics=current_topic_words,
        texts=textTokenized,
        corpus=corpus_indoBERT,
        dictionary=dictionary_indoBERT,
        coherence='c_v'
    )
    coherence_score = coherence_model_per_topic.get_coherence()

    topic_name = topic_info_indoBERT[topic_info_indoBERT['Topic'] == topic_id]['Name'].iloc[0]
    per_topic_coherence_data.append({'Topic ID': topic_id, 'Topic Name': topic_name, 'Coherence Score (c_v)': coherence_score})

# Create DataFrame for per-topic coherence
per_topic_coherence_df = pd.DataFrame(per_topic_coherence_data)

# --- Consolidate into a single table ---
# Add the overall coherence score as a summary row at the top of the DataFrame
overall_summary_row = pd.DataFrame([{'Topic ID': 'Overall', 'Topic Name': 'Overall Coherence', 'Coherence Score (c_v)': overall_coherence_score}])
consolidated_coherence_df = pd.concat([overall_summary_row, per_topic_coherence_df], ignore_index=True)

print("Consolidated Coherence Scores for IndoBERT model:")
display(consolidated_coherence_df.round(4))

"""## Display Full Topic Information Table (IndoBERT Model - Coherence Focus)

### Subtask:
Display the updated full topic information table for the `topic_model_indoBERT`.

**Reasoning**:
To display the updated full topic information table, I will output the `topic_info_indoBERT` DataFrame.
"""

topic_info_indoBERT

"""## Display Top 10 Topics (IndoBERT Model - Coherence Focus)

### Subtask:
Display a table of the top 10 topics identified by the `topic_model_indoBERT`, sorted by their document count. This will reflect the new set of topics.

**Reasoning**:
To display the top 10 topics, I will filter out the noise topic, sort the topic information by count in descending order, and then select the first 10 entries.
"""

top_10_topics_indoBERT = topic_info_indoBERT[topic_info_indoBERT['Topic'] != -1].sort_values('Count', ascending=False).head(10)
print("Top 10 Topics:")
print(top_10_topics_indoBERT)

"""## Visualize Intertopic Distance Map

### Subtask:
Generate and display the intertopic distance map for the `topic_model_indoBERT` to visualize relationships between all the newly discovered topics. This should reflect the impact of the increased `min_topic_size`.

**Reasoning**:
To visualize the relationships between the newly discovered topics, I will generate and display an intertopic distance map using the `visualize_topics()` method of the `topic_model_indoBERT` object.
"""

topic_model_indoBERT.visualize_topics()

"""## Visualize Topic Word Scores (Barchart)
### Subtask:
Generate and display the barchart visualization showing the most representative words for a selected number of topics from the `topic_model_indoBERT`.

**Reasoning**:
To visualize the most representative words for a selected number of topics, I will use the `visualize_barchart()` method from the `topic_model_indoBERT` object.
"""

topic_model_indoBERT.visualize_barchart()

"""**_____________________________________________________________________________________________________________________________**

# Task
Install the `tensorflow_text` and `tensorflow_hub` libraries to enable the use of Universal Sentence Encoder for generating embeddings.

## Install USE-related Libraries

### Subtask:
Install necessary libraries for Universal Sentence Encoder (USE) embeddings, specifically `tensorflow_text` and `tensorflow_hub`.

**Reasoning**:
The subtask requires installing `tensorflow_text` and `tensorflow_hub`. I will start by installing `tensorflow_text`.
"""

!pip install tensorflow_text

"""**Reasoning**:
I have successfully installed `tensorflow_text`. Now I will install `tensorflow_hub` as the second part of the subtask.


"""

!pip install tensorflow_hub

"""## Define USE Embedding Model

### Subtask:
Define the Universal Sentence Encoder (USE) model by loading it from `tensorflow_hub`. This step prepares the embedding component that will be used by BERTopic. We will use a multilingual USE model suitable for Indonesian text.

**Reasoning**:
To define the Universal Sentence Encoder (USE) model, I will import the necessary libraries (`tensorflow` and `tensorflow_hub`) and then load the specified multilingual USE model from `tensorflow_hub`.
"""

import tensorflow as tf
import tensorflow_hub as hub

# Load a pre-trained Universal Sentence Encoder (USE) model (multilingual)
use_model_url = "https://tfhub.dev/google/universal-sentence-encoder-multilingual/3"
use_model = hub.load(use_model_url)

print("Universal Sentence Encoder (USE) model loaded successfully.")

"""**Reasoning**:
The previous code failed with a `RuntimeError: Op type not registered 'SentencepieceOp'` when loading the Universal Sentence Encoder model. This usually happens when the `tensorflow_text` library's custom operations are not registered before the model is loaded. To fix this, I will explicitly `import tensorflow_text` at the beginning of the cell. Additionally, to comply with the instruction to use `tf.keras.layers.Lambda` for compatibility with `SentenceTransformer`, I will define a custom `SentenceTransformer`-like class that internally uses `tf.keras.layers.Lambda` to wrap the loaded TF Hub model and exposes an `encode` method as expected by `BERTopic`.


"""

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text # Explicitly import to ensure custom ops are registered
from sentence_transformers import SentenceTransformer

# Load a pre-trained Universal Sentence Encoder (USE) model (multilingual)
use_model_url = "https://tfhub.dev/google/universal-sentence-encoder-multilingual/3"

# Load the raw TF Hub module. This callable will be wrapped.
# The explicit import of `tensorflow_text` should resolve the 'SentencepieceOp' error.
tfhub_module_callable = hub.load(use_model_url)
print("TF Hub module callable loaded successfully.")

# Define a custom SentenceTransformer-compatible class that wraps the TF Hub module
# using tf.keras.layers.Lambda as specified.
class UniversalSentenceEncoderModel(SentenceTransformer):
    def __init__(self, tfhub_module_callable, model_name="UniversalSentenceEncoder"):
        super().__init__() # Initialize base SentenceTransformer
        self.tfhub_module = tfhub_module_callable
        self.model_name = model_name

        # Create a Keras model that uses the tfhub_module_callable wrapped in Lambda.
        text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)
        embeddings_output = tf.keras.layers.Lambda(lambda x: self.tfhub_module(x), name=self.model_name)(text_input)
        self.keras_encoder = tf.keras.Model(inputs=text_input, outputs=embeddings_output)

    def encode(self, sentences, batch_size=32, show_progress_bar=False, convert_to_numpy=True):
        if isinstance(sentences, str):
            sentences = [sentences]

        # Use the internal Keras model to get embeddings
        embeddings = self.keras_encoder.predict(sentences, batch_size=batch_size, verbose=0)

        if convert_to_numpy:
            return embeddings
        return tf.convert_to_tensor(embeddings)

# Instantiate the custom SentenceTransformer model
use_model = UniversalSentenceEncoderModel(tfhub_module_callable)

print("Universal Sentence Encoder (USE) model defined successfully with Lambda layer.")

"""**Reasoning**:
Now that the `use_model` is defined, the next step is to generate sentence embeddings for the preprocessed text data using this model.


"""

use_embeddings = use_model.encode(text)
print("Universal Sentence Encoder (USE) embeddings generated successfully.")

"""**Reasoning**:
To generate BERTopic topics using the Universal Sentence Encoder embeddings, I will initialize a new BERTopic model, providing the `use_model` as the embedding model. Then, I will fit this model to the preprocessed `text` data and the generated `use_embeddings`, storing the topics, probabilities, and topic information in new variables.


"""

from bertopic import BERTopic

# Initialize BERTopic model with USE embeddings
topic_model_use = BERTopic(embedding_model=use_model, min_topic_size=20)

# Fit the model to the preprocessed text and USE embeddings
topics_use, probabilities_use = topic_model_use.fit_transform(text, embeddings=use_embeddings)

# Get topic information
topic_info_use = topic_model_use.get_topic_info()

print("BERTopic model using USE embeddings trained successfully.")
print(f"Number of topics found: {len(set(topics_use)) - 1}")

"""**Reasoning**:
To prepare the data for coherence metrics using the newly trained BERTopic model with USE embeddings, I will create a gensim dictionary and corpus from the `textTokenized` data. Then, I will extract topic-word distributions and topic information from the `topic_model_use` and process the topic names to get clean words per topic.


"""

from gensim import corpora

# 1. Create a gensim dictionary from textTokenized
dictionary_use = corpora.Dictionary(textTokenized)

# 2. Create a gensim corpus from textTokenized
corpus_use = [dictionary_use.doc2bow(text) for text in textTokenized]

# 3. Extract the topic-word distributions from topic_model_use
bertopic_topics_use = topic_model_use.get_topics()

# 4. Extract the topic information from topic_model_use (already done in previous step as topic_info_use)

# 5. From topic_info_use, extract the 'Name' column, clean it to get only the words for each topic
top_words_per_topic_use = [
    [word for word in topic.split('_')[1:] if word] # Exclude topic ID and count prefix
    for topic in topic_info_use['Name']
]

# 6. Print a confirmation message
print("Data preparation for USE coherence metrics is complete.")

"""**Reasoning**:
I will calculate and display the overall and per-topic coherence scores (c_v) using `CoherenceModel` from gensim for the BERTopic model trained with USE embeddings. I will also calculate topic diversity by checking the uniqueness of the top words across all topics. The results will be summarized and printed.


"""

from gensim.models.coherencemodel import CoherenceModel
import pandas as pd

# Filter out empty lists from top_words_per_topic_use to avoid errors in coherence calculation
top_words_for_coherence_use = [words for words in top_words_per_topic_use if words]

# Calculate overall coherence score (c_v) for USE model
coherence_model_overall_use = CoherenceModel(topics=top_words_for_coherence_use, texts=textTokenized,
                                            dictionary=dictionary_use, coherence='c_v')
overall_coherence_score_use = coherence_model_overall_use.get_coherence()

# Calculate per-topic coherence scores (c_v) for USE model
per_topic_coherence_data_use = []
for i, topic_words in enumerate(top_words_per_topic_use):
    if topic_words:
        topic_coherence_model_use = CoherenceModel(topics=[topic_words], texts=textTokenized,
                                                 dictionary=dictionary_use, coherence='c_v')
        # Retrieve the topic name from topic_info_use using the original index i
        topic_name = topic_info_use['Name'].iloc[i]
        # Retrieve the actual topic ID from topic_info_use
        topic_id = topic_info_use['Topic'].iloc[i]

        per_topic_coherence_data_use.append({
            'Topic ID': topic_id,
            'Topic Name': topic_name,
            'Coherence Score (c_v)': topic_coherence_model_use.get_coherence()
        })

per_topic_coherence_df_use = pd.DataFrame(per_topic_coherence_data_use)

# --- Consolidate into a single table ---
# Add the overall coherence score as a summary row at the top of the DataFrame
overall_summary_row_use = pd.DataFrame([{'Topic ID': 'Overall', 'Topic Name': 'Overall Coherence', 'Coherence Score (c_v)': overall_coherence_score_use}])
consolidated_coherence_df_use = pd.concat([overall_summary_row_use, per_topic_coherence_df_use], ignore_index=True)

print("Consolidated Coherence Scores for USE model:")
display(consolidated_coherence_df_use.round(4))

# Calculate topic diversity for USE model
# Filter out noise topic (-1) from top_words_per_topic_use for diversity calculation
filtered_top_words_for_diversity_use = []
for i, words in enumerate(top_words_per_topic_use):
    # Assuming topic_info_use has -1 as the first topic (index 0) if it exists
    if topic_info_use['Topic'].iloc[i] != -1:
        filtered_top_words_for_diversity_use.append(words)

unique_words_across_topics_use = set()
num_total_words_use = 0
for words in filtered_top_words_for_diversity_use:
    unique_words_across_topics_use.update(words)
    num_total_words_use += len(words)

topic_diversity_use = len(unique_words_across_topics_use) / num_total_words_use if num_total_words_use > 0 else 0
print(f"\nTopic Diversity for USE model: {topic_diversity_use}")

print("\nCoherence and diversity calculation for USE model complete.")

"""**Reasoning**:
To visualize the relationships between the newly discovered topics using USE embeddings, I will generate and display an intertopic distance map using the `visualize_topics()` method of the `topic_model_use` object.


"""

!pip install bertopic

"""**Reasoning**:
Now that the `bertopic` library has been successfully installed, I will re-run the code to initialize the `use_model`, re-train `topic_model_use`, and then generate and display the intertopic distance map for the USE-based BERTopic model. This should resolve the `ModuleNotFoundError`.


"""